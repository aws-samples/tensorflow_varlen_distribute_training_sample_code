{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a1f2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20211013T113123\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ad344-226a-401e-b2f5-361cced6e34f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### for test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24a92f-754c-425e-9fd3-006488a25771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-us-west-2-687912291502/mv-poc/raw/part-00041-464b88c1-fdba-4e52-bc1e-bc73314c1e77-c000.snappy.orc ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba18332-b070-44d0-94d9-226cd00d3732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def is_json(line):\n",
    "    try:\n",
    "        json.loads(line)\n",
    "        return True\n",
    "    except json.JSONDecodeError:\n",
    "        return False\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file, 1):\n",
    "            line = line.strip()\n",
    "            line_data = json.loads(line)\n",
    "            inner_json = json.loads(line_data[\"json\"])\n",
    "            for k, v in inner_json.items():\n",
    "                #if k == \"_12004\":\n",
    "                    print(\"k,v\",k,v[1])\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "# 使用示例\n",
    "file_path = './part-00000-1afde205-86bb-4fcc-a7d3-16d0137e2baf-c000.json'  # 替换为你的文件路径\n",
    "process_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e887a-f58a-4896-978f-824fbc51015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://mv-mtg-di-for-poc/2024/06/16/08/part-01309-b0cfc571-2e0c-4445-8ecc-bbe6222436c7-c000.snappy.orc ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ac1f8-851f-4661-8265-226dcbbe3645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow.orc as orc\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "# S3 bucket 和文件路径\n",
    "bucket = \"sagemaker-us-west-2-687912291502\"\n",
    "file_key = \"mv-poc/raw3/2024/06/14/00/part-00013-1e73cc51-9b17-4439-9d71-7d505df2cae3-c000.snappy.orc\"\n",
    "# 创建 S3 客户端\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# 从 S3 下载文件\n",
    "response = s3.get_object(Bucket=bucket, Key=file_key)\n",
    "file_content = response['Body'].read()\n",
    "\n",
    "# 使用 BytesIO 创建一个类文件对象\n",
    "file_obj = BytesIO(file_content)\n",
    "\n",
    "\n",
    "# 打开 ORC 文件\n",
    "table = orc.read_table(file_obj)\n",
    "\n",
    "# 获取 schema\n",
    "schema = table.schema\n",
    "\n",
    "# 打印每个列的名称和类型\n",
    "#for field in schema:\n",
    "#    print(f\"Column Name: {field.name}, Type: {field.type}\")\n",
    "\n",
    "# 读取第一行数据（如果文件不为空）\n",
    "if table.num_rows > 0:\n",
    "    first_row = table.slice(0, 1).to_pydict()\n",
    "    print(\"\\nFirst row of data:\",first_row)\n",
    "    for column_name, values in first_row.items():\n",
    "        column_values = values[0].split(\"\\x01\")\n",
    "        print(f\"{column_name}\")\n",
    "        for column_value in column_values:\n",
    "            split_values = column_value.split(\"\\x03\")\n",
    "            print(f\"split values\",split_values)\n",
    "        #print(f\"{column_name}: {values[0]}\")\n",
    "else:\n",
    "    print(\"\\nThe file is empty.\")\n",
    "\n",
    "# 打印总行数\n",
    "print(f\"\\nTotal number of rows: {table.num_rows}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568ed8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "start_dt = datetime.datetime.now()\n",
    "print(start_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e030b7b-59f0-4869-ac72-60cf430cae8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 统计每个group_id最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c181703-1f01-40bb-a58a-f84a94ab525d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_s3_url = f\"s3://{bucket}/mv-poc/raw/\"\n",
    "#raw_s3_url = \"s3://mv-mtg-di-for-poc/2024/06/16/08/\"\n",
    "calculate_s3_url = f\"s3://{bucket}/mobivista/temp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71214d69-7688-4abd-a5dc-56aff3d03e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import (\n",
    "    ProcessingInput, \n",
    "    ProcessingOutput\n",
    ")\n",
    "\n",
    "processing_inputs = [\n",
    "        ProcessingInput(\n",
    "            source=raw_s3_url, \n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "            s3_input_mode=\"File\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "processing_outputs = [\n",
    "        ProcessingOutput(\n",
    "            output_name=\"feature_data\", \n",
    "            source=\"/opt/ml/processing/output/features\",\n",
    "            destination=calculate_s3_url,\n",
    "        )\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba6187-5413-4af5-8d3e-9d364862e312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\n",
    "             \"spark.jars\":\"s3://salunchbucket/jars/spark-tfrecord_2.12-0.3.3.jar\",\n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"60G\",\n",
    "             \"spark.executor.cores\": 40,\n",
    "             \"spark.driver.memory\": \"80G\",\n",
    "             \"spark.driver.cores\": 32,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":2,\n",
    "             \"spark.sql.hive.convertMetastoreParquet\": \"true\",\n",
    "             \"spark.driver.extraJavaOptions\": \"-Dlog4j.rootCategory=ERROR,console\",\n",
    "             \"spark.executor.extraJavaOptions\": \"-Dlog4j.rootCategory=ERROR,console\"\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"spark-feature-lenth\",\n",
    "    framework_version=\"3.1\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.12xlarge\",\n",
    "    #max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "\n",
    "spark_processor.run(\n",
    "    inputs=processing_inputs,\n",
    "    outputs=processing_outputs,\n",
    "    submit_app=\"./calculate_seq_lenth_v1.py\",\n",
    "    configuration=configuration,\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, \"pyspark\"),\n",
    "    logs=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9116822-0f74-4235-832f-9a2b66d0174d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 统计group_id唯一值的dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35fbe31-f1e5-48af-9e74-5dd63ada4d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_s3_url = f\"s3://{bucket}/mv-poc/raw/\"\n",
    "#raw_s3_url = \"s3://mv-mtg-di-for-poc/2024/06/16/08/\"\n",
    "calculate_s3_url = f\"s3://{bucket}/mobivista/temp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d0650-7a3e-48b4-9979-9ab43de24da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import (\n",
    "    ProcessingInput, \n",
    "    ProcessingOutput\n",
    ")\n",
    "\n",
    "processing_inputs = [\n",
    "        ProcessingInput(\n",
    "            source=raw_s3_url, \n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "            s3_input_mode=\"File\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "processing_outputs = [\n",
    "        ProcessingOutput(\n",
    "            output_name=\"feature_data\", \n",
    "            source=\"/opt/ml/processing/output/features\",\n",
    "            destination=calculate_s3_url,\n",
    "        )\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fbb5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\n",
    "             \"spark.jars\":\"s3://salunchbucket/jars/spark-tfrecord_2.12-0.3.3.jar\",\n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"60G\",\n",
    "             \"spark.executor.cores\": 40,\n",
    "             \"spark.driver.memory\": \"80G\",\n",
    "             \"spark.driver.cores\": 32,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":4,\n",
    "             \"spark.sql.hive.convertMetastoreParquet\": \"true\",\n",
    "             \"spark.driver.extraJavaOptions\": \"-Dlog4j.rootCategory=ERROR,console\",\n",
    "             \"spark.executor.extraJavaOptions\": \"-Dlog4j.rootCategory=ERROR,console\"\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"spark-feature-dim\",\n",
    "    framework_version=\"3.1\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.12xlarge\",\n",
    "    #max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "\n",
    "spark_processor.run(\n",
    "    inputs=processing_inputs,\n",
    "    outputs=processing_outputs,\n",
    "    submit_app=\"./calculate_dim_v3.py\",\n",
    "    configuration=configuration,\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, \"pyspark\"),\n",
    "    logs=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42b63d-2749-4874-927f-19d32e8632d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-us-west-2-687912291502/mv-poc/output/part-00000-757be5db-4d45-493f-a7b9-836104157742-c000.json ./ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8bc94-8c01-48b8-b089-293e01ce02cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://sagemaker-us-west-2-687912291502/mv-poc/output/ --human-readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ad94a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "end_dt = datetime.datetime.now()\n",
    "print(end_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2eac0-ed89-4335-a8cc-0580a763d076",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 特征编码处理及tfrecord转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5d1e2-f702-404e-aed1-36b0939b3891",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### extend prebuild pyspark process image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a8612-0a33-4b2f-bfbb-6f3f80663d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1bc758-05b1-418a-96d8-14026286ebec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-feature-process-mv-poc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6710cce-58fb-4e31-92b6-b39080aead70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.inference\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "From 153931337802.dkr.ecr.us-west-2.amazonaws.com/sagemaker-spark-processing:3.0-cpu-py39-v1.0 \n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "RUN pip3 install tensorflow==2.13.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14deb1c9-7037-4654-9ad9-482448ad56a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 153931337802.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc3a8f-3bb5-457b-98f1-9ab1cca7d22b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} -f Dockerfile.inference .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74670df7-b567-4d6b-a4ae-a43265ce63d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-feature-process-mv-poc:latest'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(\"687912291502\", \"us-west-2\", repo_name)\n",
    "inference_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b9bf91-8ddf-4dbb-ad26-66c538f01d66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 小文件合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76a651-fd4e-4e57-9b74-8f13329f385b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\n",
    "             \"spark.jars\":\"s3://salunchbucket/jars/spark-tfrecord_2.12-0.3.3.jar\",\n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"40G\",\n",
    "             \"spark.executor.cores\": 10,\n",
    "             \"spark.driver.memory\": \"30G\",\n",
    "             \"spark.driver.cores\": 40,\n",
    "             \"spark.sql.hive.convertMetastoreParquet\": \"true\",\n",
    "             \"spark.driver.extraJavaOptions\": \"-XX:-PrintGCDetails -XX:-PrintGCTimeStamps -XX:-PrintGCDateStamps -Dlog4j.rootCategory=ERROR,console\",\n",
    "             \"spark.executor.extraJavaOptions\": \"-XX:-PrintGCDetails -XX:-PrintGCTimeStamps -XX:-PrintGCDateStamps -Dlog4j.rootCategory=ERROR,console\",\n",
    "             \"spark.hadoop.fs.s3a.connection.maximum\": 1000,\n",
    "             \"spark.hadoop.fs.s3a.attempts.maximum\":10,\n",
    "             \"spark.hadoop.fs.s3a.retry.limit\":10,\n",
    "             \"spark.hadoop.fs.s3a.retry.interval\":\"1000ms\"\n",
    "             \n",
    "             \n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"Classification\": \"yarn-site\",\n",
    "        \"Properties\": {\n",
    "            \"yarn.nodemanager.local-dirs\":\"/opt/ml/output/tmp/local-dirs/\",\n",
    "            \"yarn.nodemanager.log-dirs\":\"/opt/ml/output/tmp/log-dirs/\",\n",
    "            \"yarn.nodemanager.localizer.cache.cleanup.interval-ms\": \"60000\",\n",
    "            \"yarn.nodemanager.localizer.cache.target-size-mb\": \"15120\"\n",
    "        }\n",
    "        \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"spark-merge-process\",\n",
    "    framework_version=\"3.0\",\n",
    "    role=role,\n",
    "    instance_count=6,\n",
    "    instance_type=\"ml.m5.24xlarge\",\n",
    "    image_uri=inference_image_uri,\n",
    "    volume_size_in_gb=1000\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./feature_merge.py\",\n",
    "    configuration=configuration,\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, \"pyspark\"),\n",
    "    logs=False,\n",
    "    arguments=['--num-partitions', '60000']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c370f9e-3f1a-4d97-a795-53ca7114683c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 特征转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed70b12-36e3-4de6-8e90-a9a069cd057c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_s3_url = f\"s3://{bucket}/mv-poc/raw3/\"\n",
    "#raw_s3_url = \"s3://mv-mtg-di-for-poc/2024/06/16\"\n",
    "feature_hash_dict_url = \"s3://sagemaker-us-west-2-687912291502/mv-poc/output/\"\n",
    "features_s3_url = f\"s3://{bucket}/mobivista/tf_train/features2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81eb5d57-8b2d-442f-b966-a50f010575d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import (\n",
    "    ProcessingInput, \n",
    "    ProcessingOutput\n",
    ")\n",
    "\n",
    "processing_inputs = [\n",
    "        ProcessingInput(\n",
    "            source=feature_hash_dict_url, \n",
    "            destination=\"/opt/ml/processing/input/data/\",\n",
    "            s3_input_mode=\"File\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=raw_s3_url, \n",
    "            destination=\"/opt/ml/processing/input/data/raw/\",\n",
    "            s3_input_mode=\"Pipe\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        )\n",
    "     \n",
    "    ]\n",
    "\n",
    "processing_outputs = [\n",
    "        ProcessingOutput(\n",
    "            output_name=\"feature_data\", \n",
    "            source=\"/opt/ml/processing/output/features/\",\n",
    "            destination=features_s3_url,\n",
    "        )\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81aca4-3218-45c7-a2da-f0f0ac49a114",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name spark-feature-process-2024-07-23-23-51-35-561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................_______________________________________________s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Job ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n"
     ]
    }
   ],
   "source": [
    "configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\n",
    "             \"spark.jars\":\"s3://salunchbucket/jars/spark-tfrecord_2.12-0.3.3.jar\",\n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             #\"spark.executor.memory\": \"23G\",\n",
    "             #\"spark.executor.cores\": 7,\n",
    "             #\"spark.driver.memory\": \"20G\",\n",
    "             #\"spark.driver.cores\": 7,\n",
    "             \"spark.executor.cores\": 3,\n",
    "             \"spark.executor.memory\": \"6G\",\n",
    "             \"spark.driver.memory\": \"10G\",\n",
    "             \"spark.driver.cores\": 6,\n",
    "             \"spark.dynamicAllocation.enabled\":\"true\",\n",
    "             \"spark.shuffle.service.enabled\":\"true\",\n",
    "             #\"spark.dynamicAllocation.initialExecutors\":16,\n",
    "             \"spark.sql.hive.convertMetastoreParquet\": \"true\",\n",
    "             \"spark.driver.extraJavaOptions\": \"-XX:-PrintGCDetails -XX:-PrintGCTimeStamps -XX:-PrintGCDateStamps -Dlog4j.rootCategory=ERROR,console\",\n",
    "             \"spark.executor.extraJavaOptions\": \"-XX:-PrintGCDetails -XX:-PrintGCTimeStamps -XX:-PrintGCDateStamps -Dlog4j.rootCategory=ERROR,console\",\n",
    "             \"spark.sparkContext.setLogLevel\": \"ERROR\",\n",
    "             \"spark.sql.streaming.metricsEnabled\": \"false\",            \n",
    "             \"spark.hadoop.fs.s3a.connection.maximum\": 1000,\n",
    "             \"spark.hadoop.fs.s3a.attempts.maximum\":10,\n",
    "             \"spark.hadoop.fs.s3a.retry.limit\":10,\n",
    "             \"spark.hadoop.fs.s3a.retry.interval\":\"1000ms\"\n",
    "             \n",
    "             \n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"Classification\": \"yarn-site\",\n",
    "        \"Properties\": {\n",
    "            \"yarn.nodemanager.local-dirs\":\"/opt/ml/output/tmp/local-dirs/\",\n",
    "            \"yarn.nodemanager.log-dirs\":\"/opt/ml/output/tmp/log-dirs/\",\n",
    "            \"yarn.nodemanager.localizer.cache.cleanup.interval-ms\": \"20000\",\n",
    "            \"yarn.nodemanager.localizer.cache.target-size-mb\": \"152120\"\n",
    "        }\n",
    "        \n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"spark-feature-process\",\n",
    "    framework_version=\"3.0\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.24xlarge\",\n",
    "    image_uri=inference_image_uri,\n",
    "    volume_size_in_gb=4000\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "spark_processor.run(\n",
    "    #inputs=processing_inputs,\n",
    "    outputs=processing_outputs,\n",
    "    submit_app=\"./feature_spark_v8.py\",\n",
    "    configuration=configuration,\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, \"pyspark\"),\n",
    "    logs=False,\n",
    "    arguments=['--num-partitions', '1000',\n",
    "              '--padding-lenth','10'],\n",
    "    #submit_py_files=[\"s3://sagemaker-us-west-2-687912291502/env/tensorflow_env.zip\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ca441-cd4e-4209-8fbd-b9c7f9278652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31b0a1-be6b-4070-b97d-74b05807ff3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
